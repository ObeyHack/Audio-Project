{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Studio manger is already running\n"
     ]
    }
   ],
   "source": [
    "from lightning_sdk import Studio, Machine\n",
    "from lightning_cloud.utils import add_s3_connection\n",
    "studio = Studio()\n",
    "studio.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing KenLM dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reading package lists...\\nBuilding dependency tree...\\nReading state information...\\nlibbz2-dev is already the newest version (1.0.8-2).\\nlibboost-all-dev is already the newest version (1.71.0.0ubuntu2).\\nbuild-essential is already the newest version (12.8ubuntu1.1).\\ncmake is already the newest version (3.16.3-1ubuntu1.20.04.1).\\nliblzma-dev is already the newest version (5.2.4-1ubuntu1.1).\\nzlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu1.5).\\n0 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studio.run(\"sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing KenLM toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "warning: No toolsets are configured.\nwarning: Configuring default toolset \"gcc\".\nwarning: If the default is wrong, your build may not work correctly.\nwarning: Use the \"toolset=xxxxx\" option to override our guess.\nwarning: For more configuration options, please consult\nwarning: http://boost.org/boost-build2/doc/html/bbv2/advanced/configuration.html\n...patience...\n...found 638 targets...\n...updating 4 targets...\ntesting.capture-output lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.run\n====== BEGIN OUTPUT ======\nBoost.Test WARNING: token \"lm/test.arpa\" does not correspond to the Boost.Test argument \n                    and should be placed after all Boost.Test arguments and the -- separator.\n                    For example: left_test --random -- lm/test.arpa\nRunning 6 test cases...\nunknown location(0): \u001b[4;31;49mfatal error: in \"TrieAll\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/left_test.cc(375): \u001b[1;36;49mlast checkpoint: \"TrieAll\" test entry\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"QuantTrieAll\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/left_test.cc(378): \u001b[1;36;49mlast checkpoint: \"QuantTrieAll\" test entry\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"ArrayQuantTrieAll\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/left_test.cc(381): \u001b[1;36;49mlast checkpoint: \"ArrayQuantTrieAll\" test entry\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"ArrayTrieAll\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/left_test.cc(384): \u001b[1;36;49mlast checkpoint: \"ArrayTrieAll\" test entry\u001b[0;39;49m\n\n\u001b[1;31;49m*** 4 failures are detected in the test module \"LeftTest\"\n\u001b[0;39;49m\nEXIT STATUS: 201\n====== END OUTPUT ======\n\n    LD_LIBRARY_PATH=\"/usr/bin:/usr/lib:/usr/lib32:/usr/lib64:$LD_LIBRARY_PATH\"\nexport LD_LIBRARY_PATH\n\n     \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test\"  \"lm/test.arpa\" > \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.output\" 2>&1\n    status=$?\n    echo >> \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.output\"\n    echo EXIT STATUS: $status >> \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.output\"\n    if test $status -eq 0 ; then\n        cp \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.output\" \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.run\"\n    fi\n    verbose=0\n    if test $status -ne 0 ; then\n        verbose=1\n    fi\n    if test $verbose -eq 1 ; then\n        echo ====== BEGIN OUTPUT ======\n        cat \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.output\"\n        echo ====== END OUTPUT ======\n    fi\n    exit $status\n\n...failed testing.capture-output lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.run...\ntesting.capture-output lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.run\n====== BEGIN OUTPUT ======\nBoost.Test WARNING: token \"lm/test_nounk.arpa\" does not correspond to the Boost.Test argument \n                    and should be placed after all Boost.Test arguments and the -- separator.\n                    For example: model_test --random -- lm/test_nounk.arpa\nBoost.Test WARNING: token \"lm/test.arpa\" does not correspond to the Boost.Test argument \n                    and should be placed after all Boost.Test arguments and the -- separator.\n                    For example: model_test --random -- lm/test.arpa\nRunning 12 test cases...\nunknown location(0): \u001b[4;31;49mfatal error: in \"trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"quant_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"bhiksha_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"quant_bhiksha_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"write_and_read_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"write_and_read_quant_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"write_and_read_array_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"write_and_read_quant_array_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\n\n\u001b[1;31;49m*** 8 failures are detected in the test module \"ModelTest\"\n\u001b[0;39;49m\nEXIT STATUS: 201\n====== END OUTPUT ======\n\n    LD_LIBRARY_PATH=\"/usr/bin:/usr/lib:/usr/lib32:/usr/lib64:$LD_LIBRARY_PATH\"\nexport LD_LIBRARY_PATH\n\n     \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test\"  \"lm/test_nounk.arpa\" \"lm/test.arpa\" > \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.output\" 2>&1\n    status=$?\n    echo >> \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.output\"\n    echo EXIT STATUS: $status >> \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.output\"\n    if test $status -eq 0 ; then\n        cp \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.output\" \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.run\"\n    fi\n    verbose=0\n    if test $status -ne 0 ; then\n        verbose=1\n    fi\n    if test $verbose -eq 1 ; then\n        echo ====== BEGIN OUTPUT ======\n        cat \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.output\"\n        echo ====== END OUTPUT ======\n    fi\n    exit $status\n\n...failed testing.capture-output lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.run...\n...failed updating 2 targets...\n...skipped 2 targets...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m studio\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit clone --recursive https://github.com/vchahun/kenlm.git\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcd kenlm; ./bjam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m studio\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython -m pip install pypi-kenlm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning_sdk/studio.py:217\u001b[0m, in \u001b[0;36mStudio.run\u001b[0;34m(self, *commands)\u001b[0m\n\u001b[1;32m    215\u001b[0m output, exit_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_with_exit_code(\u001b[38;5;241m*\u001b[39mcommands)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exit_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(output)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: warning: No toolsets are configured.\nwarning: Configuring default toolset \"gcc\".\nwarning: If the default is wrong, your build may not work correctly.\nwarning: Use the \"toolset=xxxxx\" option to override our guess.\nwarning: For more configuration options, please consult\nwarning: http://boost.org/boost-build2/doc/html/bbv2/advanced/configuration.html\n...patience...\n...found 638 targets...\n...updating 4 targets...\ntesting.capture-output lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.run\n====== BEGIN OUTPUT ======\nBoost.Test WARNING: token \"lm/test.arpa\" does not correspond to the Boost.Test argument \n                    and should be placed after all Boost.Test arguments and the -- separator.\n                    For example: left_test --random -- lm/test.arpa\nRunning 6 test cases...\nunknown location(0): \u001b[4;31;49mfatal error: in \"TrieAll\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/left_test.cc(375): \u001b[1;36;49mlast checkpoint: \"TrieAll\" test entry\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"QuantTrieAll\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/left_test.cc(378): \u001b[1;36;49mlast checkpoint: \"QuantTrieAll\" test entry\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"ArrayQuantTrieAll\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/left_test.cc(381): \u001b[1;36;49mlast checkpoint: \"ArrayQuantTrieAll\" test entry\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"ArrayTrieAll\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/left_test.cc(384): \u001b[1;36;49mlast checkpoint: \"ArrayTrieAll\" test entry\u001b[0;39;49m\n\n\u001b[1;31;49m*** 4 failures are detected in the test module \"LeftTest\"\n\u001b[0;39;49m\nEXIT STATUS: 201\n====== END OUTPUT ======\n\n    LD_LIBRARY_PATH=\"/usr/bin:/usr/lib:/usr/lib32:/usr/lib64:$LD_LIBRARY_PATH\"\nexport LD_LIBRARY_PATH\n\n     \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test\"  \"lm/test.arpa\" > \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.output\" 2>&1\n    status=$?\n    echo >> \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.output\"\n    echo EXIT STATUS: $status >> \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.output\"\n    if test $status -eq 0 ; then\n        cp \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.output\" \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.run\"\n    fi\n    verbose=0\n    if test $status -ne 0 ; then\n        verbose=1\n    fi\n    if test $verbose -eq 1 ; then\n        echo ====== BEGIN OUTPUT ======\n        cat \"lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.output\"\n        echo ====== END OUTPUT ======\n    fi\n    exit $status\n\n...failed testing.capture-output lm/bin/left_test.test/gcc-9/release/threading-multi/left_test.run...\ntesting.capture-output lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.run\n====== BEGIN OUTPUT ======\nBoost.Test WARNING: token \"lm/test_nounk.arpa\" does not correspond to the Boost.Test argument \n                    and should be placed after all Boost.Test arguments and the -- separator.\n                    For example: model_test --random -- lm/test_nounk.arpa\nBoost.Test WARNING: token \"lm/test.arpa\" does not correspond to the Boost.Test argument \n                    and should be placed after all Boost.Test arguments and the -- separator.\n                    For example: model_test --random -- lm/test.arpa\nRunning 12 test cases...\nunknown location(0): \u001b[4;31;49mfatal error: in \"trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"quant_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"bhiksha_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"quant_bhiksha_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"write_and_read_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"write_and_read_quant_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"write_and_read_array_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\nunknown location(0): \u001b[4;31;49mfatal error: in \"write_and_read_quant_array_trie\": lm::SpecialWordMissingException: lm/vocab.cc:237 in void lm::ngram::MissingSentenceMarker(const lm::ngram::Config&, const char*) threw SpecialWordMissingException'.\nThe ARPA file is missing </s> and the model is configured to reject these models.  Run build_binary -s to disable this check. Byte: 955 File: lm/test.arpa\u001b[0;39;49m\nlm/model_test.cc(304): \u001b[1;36;49mlast checkpoint\u001b[0;39;49m\n\n\u001b[1;31;49m*** 8 failures are detected in the test module \"ModelTest\"\n\u001b[0;39;49m\nEXIT STATUS: 201\n====== END OUTPUT ======\n\n    LD_LIBRARY_PATH=\"/usr/bin:/usr/lib:/usr/lib32:/usr/lib64:$LD_LIBRARY_PATH\"\nexport LD_LIBRARY_PATH\n\n     \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test\"  \"lm/test_nounk.arpa\" \"lm/test.arpa\" > \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.output\" 2>&1\n    status=$?\n    echo >> \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.output\"\n    echo EXIT STATUS: $status >> \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.output\"\n    if test $status -eq 0 ; then\n        cp \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.output\" \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.run\"\n    fi\n    verbose=0\n    if test $status -ne 0 ; then\n        verbose=1\n    fi\n    if test $verbose -eq 1 ; then\n        echo ====== BEGIN OUTPUT ======\n        cat \"lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.output\"\n        echo ====== END OUTPUT ======\n    fi\n    exit $status\n\n...failed testing.capture-output lm/bin/model_test.test/gcc-9/release/threading-multi/model_test.run...\n...failed updating 2 targets...\n...skipped 2 targets..."
     ]
    }
   ],
   "source": [
    "studio.run(\"git clone --recursive https://github.com/vchahun/kenlm.git\")\n",
    "studio.run(\"cd kenlm; ./bjam\")\n",
    "studio.run(\"python -m pip install pypi-kenlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--2024-08-27 17:21:22--  https://github.com/NLPH/SVLM-Hebrew-Wikipedia-Corpus/raw/master/SVLM_Hebrew_Wikipedia_Corpus.txt\\nResolving github.com (github.com)... 140.82.113.3\\nConnecting to github.com (github.com)|140.82.113.3|:443... connected.\\nHTTP request sent, awaiting response... 302 Found\\nLocation: https://raw.githubusercontent.com/NLPH/SVLM-Hebrew-Wikipedia-Corpus/master/SVLM_Hebrew_Wikipedia_Corpus.txt [following]\\n--2024-08-27 17:21:23--  https://raw.githubusercontent.com/NLPH/SVLM-Hebrew-Wikipedia-Corpus/master/SVLM_Hebrew_Wikipedia_Corpus.txt\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 3748102 (3.6M) [application/octet-stream]\\nSaving to: ‘corpus.txt’\\n\\n     0K .......... .......... .......... .......... ..........  1% 46.6M 0s\\n    50K .......... .......... .......... .......... ..........  2% 29.7M 0s\\n   100K .......... .......... .......... .......... ..........  4% 28.7M 0s\\n   150K .......... .......... .......... .......... ..........  5% 65.0M 0s\\n   200K .......... .......... .......... .......... ..........  6% 23.2M 0s\\n   250K .......... .......... .......... .......... ..........  8%  149M 0s\\n   300K .......... .......... .......... .......... ..........  9%  178M 0s\\n   350K .......... .......... .......... .......... .......... 10% 68.8M 0s\\n   400K .......... .......... .......... .......... .......... 12%  146M 0s\\n   450K .......... .......... .......... .......... .......... 13%  320M 0s\\n   500K .......... .......... .......... .......... .......... 15%  327M 0s\\n   550K .......... .......... .......... .......... .......... 16% 88.4M 0s\\n   600K .......... .......... .......... .......... .......... 17%  228M 0s\\n   650K .......... .......... .......... .......... .......... 19%  255M 0s\\n   700K .......... .......... .......... .......... .......... 20%  307M 0s\\n   750K .......... .......... .......... .......... .......... 21%  219M 0s\\n   800K .......... .......... .......... .......... .......... 23%  268M 0s\\n   850K .......... .......... .......... .......... .......... 24%  288M 0s\\n   900K .......... .......... .......... .......... .......... 25%  285M 0s\\n   950K .......... .......... .......... .......... .......... 27%  309M 0s\\n  1000K .......... .......... .......... .......... .......... 28%  306M 0s\\n  1050K .......... .......... .......... .......... .......... 30%  246M 0s\\n  1100K .......... .......... .......... .......... .......... 31%  315M 0s\\n  1150K .......... .......... .......... .......... .......... 32%  309M 0s\\n  1200K .......... .......... .......... .......... .......... 34%  318M 0s\\n  1250K .......... .......... .......... .......... .......... 35%  300M 0s\\n  1300K .......... .......... .......... .......... .......... 36%  298M 0s\\n  1350K .......... .......... .......... .......... .......... 38%  317M 0s\\n  1400K .......... .......... .......... .......... .......... 39%  270M 0s\\n  1450K .......... .......... .......... .......... .......... 40%  333M 0s\\n  1500K .......... .......... .......... .......... .......... 42%  314M 0s\\n  1550K .......... .......... .......... .......... .......... 43%  338M 0s\\n  1600K .......... .......... .......... .......... .......... 45%  279M 0s\\n  1650K .......... .......... .......... .......... .......... 46%  309M 0s\\n  1700K .......... .......... .......... .......... .......... 47%  317M 0s\\n  1750K .......... .......... .......... .......... .......... 49%  316M 0s\\n  1800K .......... .......... .......... .......... .......... 50%  265M 0s\\n  1850K .......... .......... .......... .......... .......... 51% 36.4M 0s\\n  1900K .......... .......... .......... .......... .......... 53% 36.2M 0s\\n  1950K .......... .......... .......... .......... .......... 54% 36.7M 0s\\n  2000K .......... .......... .......... .......... .......... 56% 30.4M 0s\\n  2050K .......... .......... .......... .......... .......... 57%  103M 0s\\n  2100K .......... .......... .......... .......... .......... 58% 88.7M 0s\\n  2150K .......... .......... .......... .......... .......... 60% 93.3M 0s\\n  2200K .......... .......... .......... .......... .......... 61%  114M 0s\\n  2250K .......... .......... .......... .......... .......... 62%  250M 0s\\n  2300K .......... .......... .......... .......... .......... 64%  245M 0s\\n  2350K .......... .......... .......... .......... .......... 65%  317M 0s\\n  2400K .......... .......... .......... .......... .......... 66% 63.4M 0s\\n  2450K .......... .......... .......... .......... .......... 68%  106M 0s\\n  2500K .......... .......... .......... .......... .......... 69% 85.7M 0s\\n  2550K .......... .......... .......... .......... .......... 71% 38.3M 0s\\n  2600K .......... .......... .......... .......... .......... 72%  107M 0s\\n  2650K .......... .......... .......... .......... .......... 73%  150M 0s\\n  2700K .......... .......... .......... .......... .......... 75%  101M 0s\\n  2750K .......... .......... .......... .......... .......... 76%  111M 0s\\n  2800K .......... .......... .......... .......... .......... 77%  101M 0s\\n  2850K .......... .......... .......... .......... .......... 79%  104M 0s\\n  2900K .......... .......... .......... .......... .......... 80% 96.7M 0s\\n  2950K .......... .......... .......... .......... .......... 81% 97.9M 0s\\n  3000K .......... .......... .......... .......... .......... 83% 99.5M 0s\\n  3050K .......... .......... .......... .......... .......... 84% 90.4M 0s\\n  3100K .......... .......... .......... .......... .......... 86% 98.9M 0s\\n  3150K .......... .......... .......... .......... .......... 87%  114M 0s\\n  3200K .......... .......... .......... .......... .......... 88%  122M 0s\\n  3250K .......... .......... .......... .......... .......... 90%  140M 0s\\n  3300K .......... .......... .......... .......... .......... 91% 80.9M 0s\\n  3350K .......... .......... .......... .......... .......... 92%  145M 0s\\n  3400K .......... .......... .......... .......... .......... 94%  160M 0s\\n  3450K .......... .......... .......... .......... .......... 95%  182M 0s\\n  3500K .......... .......... .......... .......... .......... 96%  120M 0s\\n  3550K .......... .......... .......... .......... .......... 98%  212M 0s\\n  3600K .......... .......... .......... .......... .......... 99%  179M 0s\\n  3650K ..........                                            100%  304M=0.03s\\n\\n2024-08-27 17:21:23 (105 MB/s) - ‘corpus.txt’ saved [3748102/3748102]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studio.run(\"wget -c https://github.com/NLPH/SVLM-Hebrew-Wikipedia-Corpus/raw/master/SVLM_Hebrew_Wikipedia_Corpus.txt -O corpus.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "with open('corpus.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "output = []\n",
    "for line in lines:\n",
    "    for sentence in nltk.sent_tokenize(line):\n",
    "        output.append(' '.join(nltk.word_tokenize(sentence)).lower())\n",
    "\n",
    "with open('corpus_preprocessed.txt', 'w') as f:\n",
    "    f.write('\\n'.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'=== 1/5 Counting and sorting n-grams ===\\nReading /teamspace/studios/this_studio/corpus_preprocessed.txt\\n----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\\n****************************************************************************************************\\n=== 2/5 Calculating and sorting adjusted counts ===\\nChain sizes: 1:261468 2:1285314432 3:2409964800 4:3855943424 5:5623250944\\nStatistics:\\n1 21789 D1=0.331002 D2=1.15875 D3+=1.94464\\n2 240365 D1=0.818674 D2=1.17532 D3+=1.3876\\n3 338491 D1=0.93551 D2=1.31438 D3+=1.462\\n4 329334 D1=0.977548 D2=1.43285 D3+=1.33817\\n5 289228 D1=0.987732 D2=1.58096 D3+=1.51056\\nMemory estimate for binary LM:\\ntype       kB\\nprobing 26923 assuming -p 1.5\\nprobing 32329 assuming -r models -p 1.5\\ntrie    12888 without quantization\\ntrie     6872 assuming -q 8 -b 8 quantization \\ntrie    11558 assuming -a 22 array pointer compression\\ntrie     5542 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\\n=== 3/5 Calculating and sorting initial probabilities ===\\nChain sizes: 1:261468 2:3845840 3:6769820 4:7904016 5:8098384\\n=== 4/5 Calculating and writing order-interpolated probabilities ===\\nChain sizes: 1:261468 2:3845840 3:6769820 4:7904016 5:8098384\\nChain sizes: 1:261468 2:3845840 3:6769820 4:7904016 5:8098384\\n=== 5/5 Writing ARPA model ===\\nName:lmplz\\tVmPeak:13160292 kB\\tVmRSS:40784 kB\\tRSSMax:0 kB\\tuser:0\\tsys:0\\tCPU:0\\treal:2.00317'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studio.run(\"kenlm/bin/lmplz -o 5 < corpus_preprocessed.txt > model.arpa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the model to binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reading model.arpa\\n----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\\n****************************************************************************************************\\nSUCCESS'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studio.run(\"kenlm/bin/build_binary model.arpa model.bin.lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[NeMo I 2024-08-27 17:48:03 create_lexicon_from_arpa:62] Writing Lexicon file to: ./model.lexicon...'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studio.run(\"wget -c https://github.com/NVIDIA/NeMo/raw/main/scripts/asr_language_modeling/ngram_lm/create_lexicon_from_arpa.py -O create_lexicon_from_arpa.py\")\n",
    "studio.run(\"python create_lexicon_from_arpa.py --arpa model.arpa --dst .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading to s3 backet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cp: cannot create regular file '/teamspace/s3_connections/audio-speech-hebrew/language_model/model.bin': Read-only file system",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/teamspace/s3_connections/audio-speech-hebrew\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m add_s3_connection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio-speech-hebrew\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mstudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcp model.bin /teamspace/s3_connections/audio-speech-hebrew/language_model/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning_sdk/studio.py:217\u001b[0m, in \u001b[0;36mStudio.run\u001b[0;34m(self, *commands)\u001b[0m\n\u001b[1;32m    215\u001b[0m output, exit_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_with_exit_code(\u001b[38;5;241m*\u001b[39mcommands)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exit_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(output)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cp: cannot create regular file '/teamspace/s3_connections/audio-speech-hebrew/language_model/model.bin': Read-only file system"
     ]
    }
   ],
   "source": [
    "path = \"/teamspace/s3_connections/audio-speech-hebrew\"\n",
    "add_s3_connection(\"audio-speech-hebrew\")\n",
    "\n",
    "studio.run(\"cp model.bin.lm /teamspace/s3_connections/audio-speech-hebrew/language_model/\")\n",
    "studio.run(\"cp model.lexicon /teamspace/s3_connections/audio-speech-hebrew/language_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing the unnecessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studio.run(\"rm -rf model.arpa\")\n",
    "studio.run(\"rm -rf corpus.txt\")\n",
    "studio.run(\"rm -rf corpus_preprocessed.txt\")\n",
    "studio.run(\"rm -rf create_lexicon_from_arpa.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.032098770141602\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "model = kenlm.LanguageModel('model.bin')\n",
    "score = model.score('שלום מה הסתברות שלי')\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
